{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert2Images and Train\n",
    "- Image Conversion taken from `https://github.com/thongonary/LCDJets/blob/master/Transform.py` - __tested!__\n",
    "- For each row, produce an image from a matrix of features \n",
    "- Train... __testing...__\n",
    "\n",
    "- The github repo contains a env setup bash script: `source analytix.sh analytix` \n",
    "- To launch, see the env setup in preprocessing_... and then:\n",
    "```\n",
    "pyspark --master yarn --packages org.diana-hep:root4j:0.1.6 --jars /afs/cern.ch/work/v/vkhriste/public/spark-root_2.11-0.1.16.jar --executor-memory 4G  --executor-cores 4 --num-executors 25 --files $KRB5CCNAME#krbcache --conf spark.executorEnv.KRB5CCNAME='FILE:$PWD/krbcache' --conf spark.driver.extraClassPath=\"/usr/lib/hadoop/EOSfs.jar\" --conf spark.executorEnv.PYTHONPATH=\"/afs/cern.ch/user/v/vkhriste/.local/lib/python2.7/site-packages\" --conf spark.executorEnv.LD_LIBRARY_PATH=\"/cvmfs/sft.cern.ch/lcg/views/LCG_88/x86_64-slc6-gcc49-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_88/x86_64-slc6-gcc49-opt/lib:/cvmfs/sft.cern.ch/lcg/contrib/gcc/4.9/x86_64-slc6/lib64\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Preprocessed Data\n",
    "- QCD samples\n",
    "- TTBar samples\n",
    "- W+Jets samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'/tmp/spark-e2e09dcd-7766-46ad-9887-5383939914b7/userFiles-fba95353-23a5-492a-8b3e-1c30c5797cdf', '', '/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip', '/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python', '/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages', '/opt/rh/python27/root/usr/lib64/python27.zip', '/opt/rh/python27/root/usr/lib64/python2.7', '/opt/rh/python27/root/usr/lib64/python2.7/plat-linux2', '/opt/rh/python27/root/usr/lib64/python2.7/lib-tk', '/opt/rh/python27/root/usr/lib64/python2.7/lib-old', '/opt/rh/python27/root/usr/lib64/python2.7/lib-dynload', '/afs/cern.ch/user/v/vkhriste/.local/lib/python2.7/site-packages', '/opt/rh/python27/root/usr/lib64/python2.7/site-packages', '/opt/rh/python27/root/usr/lib/python2.7/site-packages', '/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/IPython/extensions', '/afs/cern.ch/user/v/vkhriste/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_features = sqlContext.read.format(\"parquet\").load(\"hdfs:/cms/bigdatasci/vkhriste/data/qcd_preprocessing/test_1530_12122017\")\n",
    "ttbar_features = sqlContext.read.format(\"parquet\").load(\"hdfs:/cms/bigdatasci/vkhriste/data/ttbar_preprocessing/test_2200_11122017\")\n",
    "wjets_features = sqlContext.read.format(\"parquet\").load(\"hdfs:/cms/bigdatasci/vkhriste/data/wjets_preprocessing/test_1030_12122017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           hfeatures|           lfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[100.228340148925...|[WrappedArray(99....|\n",
      "|[42.2832412719726...|[WrappedArray(34....|\n",
      "|[0.0, 12.81347084...|[WrappedArray(204...|\n",
      "|[0.0, 65.22929382...|[WrappedArray(179...|\n",
      "|[0.0, 16.83046531...|[WrappedArray(203...|\n",
      "|[0.0, 4.743223190...|[WrappedArray(50....|\n",
      "|[50.4907798767089...|[WrappedArray(51....|\n",
      "|[0.0, 53.21599578...|[WrappedArray(61....|\n",
      "|[0.0, 2.571652889...|[WrappedArray(46....|\n",
      "|[41.7809562683105...|[WrappedArray(29....|\n",
      "|[58.6198959350585...|[WrappedArray(40....|\n",
      "|[0.0, 32.54497146...|[WrappedArray(28....|\n",
      "|[53.9156837463378...|[WrappedArray(668...|\n",
      "|[84.5948333740234...|[WrappedArray(127...|\n",
      "|[0.0, 25.22752761...|[WrappedArray(70....|\n",
      "|[96.4648742675781...|[WrappedArray(70....|\n",
      "|[0.0, 18.52447128...|[WrappedArray(360...|\n",
      "|[0.0, 9.786623001...|[WrappedArray(119...|\n",
      "|[184.848709106445...|[WrappedArray(64....|\n",
      "|[0.0, 59.09547805...|[WrappedArray(74....|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcd_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           hfeatures|           lfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[962.852188110351...|[WrappedArray(249...|\n",
      "|[352.717723846435...|[WrappedArray(167...|\n",
      "|[242.214546203613...|[WrappedArray(67....|\n",
      "|[100.947238922119...|[WrappedArray(28....|\n",
      "|[59.9977836608886...|[WrappedArray(64....|\n",
      "|[493.494388580322...|[WrappedArray(52....|\n",
      "|[267.234786987304...|[WrappedArray(241...|\n",
      "|[206.327716827392...|[WrappedArray(63....|\n",
      "|[383.656970977783...|[WrappedArray(135...|\n",
      "|[173.195686340332...|[WrappedArray(76....|\n",
      "|[69.5480575561523...|[WrappedArray(40....|\n",
      "|[181.502624511718...|[WrappedArray(46....|\n",
      "|[333.205291748046...|[WrappedArray(97....|\n",
      "|[73.81298828125, ...|[WrappedArray(481...|\n",
      "|[109.182903289794...|[WrappedArray(75....|\n",
      "|[239.590858459472...|[WrappedArray(106...|\n",
      "|[645.563568115234...|[WrappedArray(575...|\n",
      "|[125.917373657226...|[WrappedArray(95....|\n",
      "|[440.674312591552...|[WrappedArray(28....|\n",
      "|[514.676177978515...|[WrappedArray(119...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ttbar_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           hfeatures|           lfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[0.0, 52.68512344...|[WrappedArray(185...|\n",
      "|[0.0, 48.29389953...|[WrappedArray(349...|\n",
      "|[0.0, 34.79087448...|[WrappedArray(220...|\n",
      "|[0.0, 21.25173377...|[WrappedArray(285...|\n",
      "|[0.0, 57.50206756...|[WrappedArray(148...|\n",
      "|[46.7039146423339...|[WrappedArray(160...|\n",
      "|[0.0, 15.00487327...|[WrappedArray(45....|\n",
      "|[0.0, 79.79054260...|[WrappedArray(104...|\n",
      "|[0.0, 41.56198120...|[WrappedArray(248...|\n",
      "|[85.4285049438476...|[WrappedArray(199...|\n",
      "|[0.0, 34.41744995...|[WrappedArray(47....|\n",
      "|[0.0, 45.74290084...|[WrappedArray(43....|\n",
      "|[0.0, 16.20509147...|[WrappedArray(35....|\n",
      "|[0.0, 29.39954376...|[WrappedArray(44....|\n",
      "|[0.0, 62.23700714...|[WrappedArray(174...|\n",
      "|[0.0, 37.56029129...|[WrappedArray(27....|\n",
      "|[0.0, 20.01223564...|[WrappedArray(37....|\n",
      "|[0.0, 15.26028919...|[WrappedArray(37....|\n",
      "|[0.0, 41.44536590...|[WrappedArray(59....|\n",
      "|[0.0, 29.08719253...|[WrappedArray(689...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wjets_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Build up the functionality\n",
    "- various imports\n",
    "- conversion function\n",
    "- __Note:__ adding local site-package only needed for a driver!\n",
    "- __Note:__ `reload(six)` is used because somehow default location of six is chosen (most likely during the import sys, six is loaded as well...) -> since it has been loaded, updating the `sys.path` will not influence that... __Have to reload the module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/afs/cern.ch/user/v/vkhriste/.local/lib/python2.7/site-packages\")\n",
    "import six\n",
    "print six.__version__\n",
    "reload(six)\n",
    "print six.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "1.13.3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time, os\n",
    "from pyspark.sql import Row\n",
    "from scipy import misc\n",
    "from skimage import draw\n",
    "\n",
    "print matplotlib.__version__\n",
    "print np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/afs/cern.ch/user/v/vkhriste/.local/lib/python2.7/site-packages',\n",
       " u'/tmp/spark-e2e09dcd-7766-46ad-9887-5383939914b7/userFiles-fba95353-23a5-492a-8b3e-1c30c5797cdf',\n",
       " '',\n",
       " '/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip',\n",
       " '/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python',\n",
       " '/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages',\n",
       " '/opt/rh/python27/root/usr/lib64/python27.zip',\n",
       " '/opt/rh/python27/root/usr/lib64/python2.7',\n",
       " '/opt/rh/python27/root/usr/lib64/python2.7/plat-linux2',\n",
       " '/opt/rh/python27/root/usr/lib64/python2.7/lib-tk',\n",
       " '/opt/rh/python27/root/usr/lib64/python2.7/lib-old',\n",
       " '/opt/rh/python27/root/usr/lib64/python2.7/lib-dynload',\n",
       " '/afs/cern.ch/user/v/vkhriste/.local/lib/python2.7/site-packages',\n",
       " '/opt/rh/python27/root/usr/lib64/python2.7/site-packages',\n",
       " '/opt/rh/python27/root/usr/lib/python2.7/site-packages',\n",
       " '/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/afs/cern.ch/user/v/vkhriste/.ipython']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skimage import draw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors, Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'yellow', 'blue', 'green', 'green', 'black']\n",
      "[array([ 1.,  0.,  0.]), array([ 1.,  1.,  0.]), array([ 0.,  0.,  1.]), array([ 0.        ,  0.50196078,  0.        ]), array([ 0.        ,  0.50196078,  0.        ]), array([ 0.,  0.,  0.])]\n",
      "[4, 0, 3, 5, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "feature_variables = ['Energy', 'Px', 'Py', 'Pz', 'Pt', 'Eta', 'Phi', \n",
    "                    'vtxX', 'vtxY', 'vtxZ','ChPFIso', 'GammaPFIso', 'NeuPFIso',\n",
    "                    'isChHad', 'isNeuHad', 'isGamma', 'isEle',  'isMu', \n",
    "                        #'Charge'\n",
    "           ]\n",
    "\n",
    "\n",
    "# In[259]:\n",
    "\n",
    "colors = {'isMu' : 'green',\n",
    "        'isEle': 'green',\n",
    "         'isGamma':'blue',\n",
    "         'isChHad' : 'red',\n",
    "         'isNeuHad': 'yellow'}\n",
    "\n",
    "shapes = {'isMu' : 5,\n",
    "          'isEle': 5,\n",
    "          'isGamma':3,\n",
    "          'isChHad' : 4,\n",
    "          'isNeuHad': 0}\n",
    "\n",
    "c_colors = [colors[k] for k in feature_variables[13:]]+['black']\n",
    "cc_colors = [np.asarray(matplotlib.colors.to_rgb(k)) for k in c_colors]\n",
    "cc_shapes = [shapes[k] for k in feature_variables[13:]]+[0]\n",
    "\n",
    "print c_colors\n",
    "print cc_colors\n",
    "print cc_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(image):\n",
    "    fig = plt.figure(frameon=False)\n",
    "    plt.imshow(image.swapaxes(0,1))\n",
    "    plt.axis('off')\n",
    "    plt.savefig('fig.png', dpi=100, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "def create3D(data):\n",
    "    max_eta = 5\n",
    "    max_phi = np.pi\n",
    "    res= 100\n",
    "    neta = int(max_eta*res)\n",
    "    nphi = int(max_phi*res)\n",
    "    eeta = 2.*max_eta / float(neta)\n",
    "    ephi = 2.*max_phi / float(nphi)\n",
    "    def ieta( eta ): return (eta+max_eta) / eeta\n",
    "    def iphi(phi) : return (phi+max_phi) / ephi\n",
    "    blend = 0.3\n",
    "    image = np.ones((neta,nphi,3), dtype = np.float32)\n",
    "    \n",
    "    before_loop = time.time()\n",
    "    for ip in range(data.shape[0]):\n",
    "        p_data = data[ip,:]\n",
    "        eta = p_data[0]\n",
    "        phi = p_data[1]\n",
    "        if eta==0 and phi==0: \n",
    "            #print ip\n",
    "            continue\n",
    "        #pT = p_data[2]\n",
    "        #lpT = min(max(np.log(pT)/5.,0.001), 10)*res/2.\n",
    "        lpT = p_data[2]\n",
    "        ptype = int(p_data[3])\n",
    "        c = cc_colors[ ptype ]\n",
    "        s = cc_shapes[ ptype ]\n",
    "        R = lpT * res/1.5\n",
    "        iee = ieta(eta)\n",
    "        ip0 = iphi(phi)\n",
    "        ip1 = iphi(phi+2*np.pi)\n",
    "        ip2 = iphi(phi-2*np.pi)\n",
    "        \n",
    "        if s==0:\n",
    "            xi0,yi0 = draw.circle(  iee, ip0,radius=R, shape=image.shape[:2])\n",
    "            xi1,yi1 = draw.circle( iee, ip1, radius=R, shape=image.shape[:2])\n",
    "            xi2,yi2 = draw.circle( iee, ip2, radius=R, shape=image.shape[:2]) \n",
    "            #if ptype == 5:\n",
    "            #    print \"MET\",eta,phi\n",
    "        else:\n",
    "            nv = s\n",
    "            vx = [iee + R*np.cos(ang) for ang in np.arange(0,2*np.pi, 2*np.pi/nv)]\n",
    "            vy = [ip0 + R*np.sin(ang) for ang in np.arange(0,2*np.pi, 2*np.pi/nv)]\n",
    "            vy1 = [ip1 + R*np.sin(ang) for ang in np.arange(0,2*np.pi, 2*np.pi/nv)]\n",
    "            vy2 = [ip2 + R*np.sin(ang) for ang in np.arange(0,2*np.pi, 2*np.pi/nv)]\n",
    "            xi0,yi0 = draw.polygon( vx, vy , shape=image.shape[:2])\n",
    "            xi1,yi1 = draw.polygon( vx, vy1 , shape=image.shape[:2])\n",
    "            xi2,yi2 = draw.polygon( vx, vy2 , shape=image.shape[:2])\n",
    "            \n",
    "        xi = np.concatenate((xi0,xi1,xi2))\n",
    "        yi = np.concatenate((yi0,yi1,yi2))\n",
    "        image[xi,yi,:] = (image[xi,yi,:] *(1-blend)) + (c*blend)\n",
    "    after_loop = time.time()\n",
    "    print \"Time to process the loop inside create3D : %3.3f\" % (after_loop - before_loop)\n",
    "    return Vectors.dense(image.reshape((neta*nphi*3)))\n",
    "\n",
    "\n",
    "def convert2image(row):\n",
    "    \"\"\"Assume that a row contains a non-empty 2D matrix of features\"\"\"\n",
    "    start = time.time()\n",
    "    #\n",
    "    # this is for avoiding clashes on CC7 and SLC6 of the python versions\n",
    "    #\n",
    "    if np.__version__!=\"1.13.3\":\n",
    "        import sys\n",
    "        sys.path.insert(0, \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages\")\n",
    "        reload(np)\n",
    "    \n",
    "    lmat = np.asarray(row.lfeatures, dtype=np.float32)\n",
    "    hmat = np.asarray(row.hfeatures, dtype=np.float32)\n",
    "    \n",
    "    # low level features\n",
    "    l_reduced = np.asarray(np.zeros((lmat.shape[0], 4)))\n",
    "    l_reduced[:, 0] = lmat[:, 5]\n",
    "    l_reduced[:, 1] = lmat[:, 6]\n",
    "    l_reduced[:, 2] = np.minimum(np.log(np.maximum(lmat[:, 4], 1.001))/5., 10)\n",
    "    l_reduced[:, 3] = np.argmax(lmat[:, 13:], axis=-1)\n",
    "    \n",
    "    # high level features\n",
    "    h_reduced = np.zeros( (1, 4))\n",
    "    h_reduced[0,2] = np.minimum(np.maximum(np.log(hmat[1])/5.,0.001), 10) # MET\n",
    "    h_reduced[0,1] = hmat[2] # MET-phi\n",
    "    h_reduced[0,3] = int(5) ## met type\n",
    "    \n",
    "    # concatenate the high and low level features\n",
    "    reduced = np.concatenate((l_reduced, h_reduced), axis=0)\n",
    "    \n",
    "    # geneate the image (as a 3D matrix)\n",
    "    before_create3D = time.time()\n",
    "    img = create3D(reduced)\n",
    "    \n",
    "    before_tolist = time.time()\n",
    "    l = img\n",
    "    end = time.time()\n",
    "    \n",
    "    print \"Tiem to procss a Row before create3D: %3.3f\" % (before_create3D - start)\n",
    "    print \"Time to process a Row: %3.3f\" % (end - start)\n",
    "    print \"Time to run tolist: %3.3f\" % (end - before_tolist)\n",
    "    \n",
    "    return Row(image=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN the pipeline: Get the images from preprocessed features\n",
    "- qcd \n",
    "- ttbar\n",
    "- wjets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processing Time: 2 seconds\n"
     ]
    }
   ],
   "source": [
    "before = time.time()\n",
    "qcd_images = qcd_features\\\n",
    "    .rdd\\\n",
    "    .map(convert2image)\\\n",
    "    .toDF()\n",
    "after = time.time()\n",
    "print(\"Total Processing Time: %d seconds\" % int(after - before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processing Time: 12 seconds\n"
     ]
    }
   ],
   "source": [
    "before = time.time()\n",
    "ttbar_images = ttbar_features\\\n",
    "    .rdd\\\n",
    "    .map(convert2image)\\\n",
    "    .toDF()\n",
    "after = time.time()\n",
    "print(\"Total Processing Time: %d seconds\" % int(after - before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processing Time: 1 seconds\n"
     ]
    }
   ],
   "source": [
    "before = time.time()\n",
    "wjets_images = wjets_features\\\n",
    "    .rdd\\\n",
    "    .map(convert2image)\\\n",
    "    .toDF()\n",
    "after = time.time()\n",
    "print(\"Total Processing Time: %d seconds\" % int(after - before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qcd = qcd_features.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process the loop inside create3D : 0.137\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.144\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.191\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.196\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.115\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.121\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.190\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.194\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.237\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.242\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.263\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.267\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.262\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.267\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.362\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.367\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.148\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.153\n",
      "Time to run tolist: 0.000\n",
      "Time to process the loop inside create3D : 0.163\n",
      "Tiem to procss a Row before create3D: 0.002\n",
      "Time to process a Row: 0.168\n",
      "Time to run tolist: 0.000\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n",
      "(471000,)\n"
     ]
    }
   ],
   "source": [
    "new_rows = []\n",
    "for row in sample_qcd:\n",
    "    new_rows.append(convert2image(row))\n",
    "for row in new_rows:\n",
    "    print row.image.array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the schema for the produced images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qcd_images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ttbar_images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wjets_images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the generated images\n",
    "- qcd\n",
    "- ttbar\n",
    "- wjets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_test_images = qcd_images.take(10)\n",
    "ttbar_test_images = ttbar_images.take(10)\n",
    "wjets_test_images = wjets_images.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in qcd_test_images:\n",
    "    img = x.image.array.reshape((500, 314, 3))\n",
    "    showImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ttbar_test_images:\n",
    "    img = x.image.array.reshape((500, 314, 3))\n",
    "    showImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in wjets_test_images:\n",
    "    img = x.image.array.reshape((500, 314, 3))\n",
    "    showImage(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare the data for training\n",
    "- Encode the labels\n",
    "- split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "#from pyspark.ml.feature import StandardScaler\n",
    "#from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.ml.feature import OneHotEncoder\n",
    "#from pyspark.ml.feature import MinMaxScaler\n",
    "##from pyspark.ml.feature import StringIndexer\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#from distkeras.trainers import *\n",
    "#from distkeras.predictors import *\n",
    "#from distkeras.transformers import *\n",
    "#from distkeras.evaluators import *\n",
    "#from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# assign labels in alphabetical order\n",
    "#\n",
    "qcd_label = 0\n",
    "ttbar_label = 1\n",
    "wjets_label = 2\n",
    "\n",
    "#\n",
    "# create new columns for that\n",
    "#\n",
    "qcd_sample = qcd_images.withColumn(\"label\", lit(qcd_label))\n",
    "ttbar_sample = ttbar_images.withColumn(\"label\", lit(ttbar_label))\n",
    "wjets_sample = wjets_images.withColumn(\"label\", lit(wjets_label))\n",
    "\n",
    "#\n",
    "# combine all of that in just 1 data frame\n",
    "#\n",
    "data_sample = qcd_sample.union(ttbar_sample).union(wjets_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's save the data at this point to hdfs\n",
    "#\n",
    "data_sample.write.parquet(\"hdfs:/cms/bigdatasci/vkhriste/data/images_all/test_17122017_0\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# perform a quick (__not that quick__) count of number of rows\n",
    "#\n",
    "#data_sample.count()\n",
    "\n",
    "#\n",
    "# Should be 39447519 events total\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 15, p05151113997207.cern.ch, executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 163, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 54, in read_command\n    command = serializer._read_with_length(file)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 434, in loads\n    return pickle.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/distkeras/utils.py\", line 5, in <module>\n    from keras import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\n    from . import utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\n    from . import conv_utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/conv_utils.py\", line 3, in <module>\n    from .. import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/__init__.py\", line 83, in <module>\n    from .tensorflow_backend import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\n    from tensorflow.python import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 163, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 54, in read_command\n    command = serializer._read_with_length(file)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 434, in loads\n    return pickle.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/distkeras/utils.py\", line 5, in <module>\n    from keras import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\n    from . import utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\n    from . import conv_utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/conv_utils.py\", line 3, in <module>\n    from .. import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/__init__.py\", line 83, in <module>\n    from .tensorflow_backend import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\n    from tensorflow.python import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0822b818b61a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label_encoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata_sample_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/afs/cern.ch/user/v/vkhriste/.local/lib/python2.7/site-packages/distkeras/transformers.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataframe)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0mDataframe\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhot\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 15, p05151113997207.cern.ch, executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 163, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 54, in read_command\n    command = serializer._read_with_length(file)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 434, in loads\n    return pickle.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/distkeras/utils.py\", line 5, in <module>\n    from keras import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\n    from . import utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\n    from . import conv_utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/conv_utils.py\", line 3, in <module>\n    from .. import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/__init__.py\", line 83, in <module>\n    from .tensorflow_backend import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\n    from tensorflow.python import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 163, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/worker.py\", line 54, in read_command\n    command = serializer._read_with_length(file)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 169, in _read_with_length\n    return self.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/public/spark-2.1.0-bin-hadoop2.6/python/pyspark/serializers.py\", line 434, in loads\n    return pickle.loads(obj)\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/distkeras/utils.py\", line 5, in <module>\n    from keras import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\n    from . import utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\n    from . import conv_utils\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/utils/conv_utils.py\", line 3, in <module>\n    from .. import backend as K\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/__init__.py\", line 83, in <module>\n    from .tensorflow_backend import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\n    from tensorflow.python import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /afs/cern.ch/user/p/pkothuri/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we have 3 classes for our Classifier\n",
    "#\n",
    "num_classes = 3\n",
    "encoder = OneHotTransformer(num_classes, input_col=\"label\", output_col=\"label_encoded\")\n",
    "data_sample_encoded = encoder.transform(data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
